# (Mathematical) Physics, Probability Theory

This folder is divided into several subfolders and papers that are just too broad to be classified into subcategories. The main theme of this folder is to understand deep learning by using concepts and techniques/tools from (mathematical) physics and probability theory. These include, but are not limited to, random matrix theory (RMT), mean-field theory (MFT), spin glass...etc.



## Notes

- Papers with MFT-based approaches are moved to a separate subfolder *MFT*



## Papers

- The Loss Surfaces of Multilayer Networks (Choromanska et al., *AISTATS'15*)
  - 
- Nonlinear random matrix theory for deep learning (Pennington & Worah, *NIPS'17*)
  - Extended version of the same title by the same authors appeared in the *Journal of Statistical Mechanics: Theory and Experiment (2019)*
  - 
- Traditional and Heavy Tailed Self Regularization in Neural Network Models (Martin & Mahoney, *ICML'19*)
  - Extended version "Implicit Self-Regularization in Deep Neural Networks: Evidence from Random Matrix Theory and Implications for Learning" by the same authors available on *arXiv*.
  - 

- 



## Additional Resources

- Topics in Random Matrix Theory (Terence Tao, 2012)
  - AMS Graduate Studies in Mathematics, Vol. 132
  - 

