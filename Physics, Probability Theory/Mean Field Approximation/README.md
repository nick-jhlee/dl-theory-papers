# Mean-field theory (MFT)

This folder contains papers that try to understand the training dynamics of deep learning from the lens of MFT.  Refer to *Background* folder for some background materials on MFT.



## A mean field view of the landscape of two-layer neural networks (Mei et al., PNAS'18)



## On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport (Chizat & Bach, NIPS'18)

- A bit different from the other works presented here. However, the basic idea of MFT (approximating high-dimensional system by averaging over particles...) applies here; see the last subsection of Sec 1.2
- 



## Mean Field Analysis of Neural Networks: A Central Limit Theorem (Sirignano & Spiliopoulos, 2020; Stochastic Processes and their Applications)



## Mean Field Analysis of Neural Networks: A Law of Large Numbers (Sirignano & Spiliopoulos, 2020; SIAM Journal on Applied Mathematics)



## Mean Field Analysis of Deep Neural Networks (Sirignano & Spiliopoulos, 2020; Mathematics of Operations Research)

