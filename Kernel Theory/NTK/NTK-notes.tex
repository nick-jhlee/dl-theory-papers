\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage[russian, english]{babel}

\usepackage{kotex, amsmath, amssymb, amsthm, mathtools, enumitem, systeme, bbm, physics}
  % use Ko-tex, math equation, symbol, theorem, enumerate
\usepackage[onehalfspacing]{setspace} % line spacing
\usepackage[all]{xy} % draw diagram
\usepackage{adjustbox}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{float}
\usepackage{enumitem}
\usepackage{indentfirst}
\usepackage{collectbox}
\usepackage[margin=1in]{geometry}
\usepackage{tikz-cd}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{tcolorbox}
\usepackage{authblk}

\renewcommand{\qedsymbol}{$\blacksquare$} % Q.E.D. symbol
\newcommand{\mybox}{%
    \collectbox{%
        \setlength{\fboxsep}{1pt}%
        \fbox{\BOXCONTENT}%
    }%
}

%\makeatletter
%\renewcommand\AB@affilsepx{\hfill \protect\Affilfont}
%\makeatother

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{problem}{Problem}

\newtheorem*{claim}{Claim}

\theoremstyle{remark}
\newtheorem{remark}{Remark}
\newtheorem*{note}{Note}
\newtheorem*{exer}{Exercise}
\newtheorem* {prob}{Problem}


\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\DeclareMathOperator*{\argmax}{\arg\!\max}
\DeclareMathOperator*{\argmin}{\arg\!\min}

\newcommand{\sgn}{\mathrm{sgn}}


\begin{document}

\title{%
  \huge Notes on "Neural Tangent Kernel: Convergence and Generalization in Neural Networks" by Jacot et al., 2018
  }
\author[1,2]{Junghyun Lee}
\affil[1]{Department of Mathematical Sciences}
\affil[2]{School of Computing}
\affil[ ]{KAIST, Daejeon, South Korea}
\affil[ ]{\it jh\_lee00@kaist.ac.kr}
%\date{\today}
\maketitle

This note will cover the first paper\cite{NTK} that introduced the neural tangent kernel (NTK).
In my opinion, the paper is written in a very mathematically (and physically\footnote{as in physics}) intricate manner, but with not so much intuition.
Many blogs and articles \cite{NTK-Rajat, NTK-CMU, NTK-Huszar, NTK-oxford} provide excellent expositions to the intuition behind NTKs, but not much materials are available for understanding the paper as it is with all the mathematical subtleties\footnote{The closest resource was \cite{NTK-Jinwoo}.}.

This note is meant to serve as a companion to reading \cite{NTK}; this note will supplement the paper with logical gaps, missing mathematical definitions, missing calculation steps...etc.
(Some of the elementary proofs will be left as exercises, although the solutions will be provided at the Appendix)
Also, borrowing from the mentioned blogs and articles, this note will also cover the intuition behind NTK and some of its implications in the connection between kernel theory and deep learning.



\section{Basic Setup (ANN)}
Here, we only consider fully-connected networks of depth $L$, as shown in Figure ?.


\section{Deep Learning as Optimization over Function Space}
\subsection{Essential Functional Analysis}
For each given parameter vector $\theta \in \mathbb{R}^P$, the corresponding neural network $f_\theta$ can be regarded as an element of the function space $\mathcal{F} = \{f : \mathbb{R}^{n_0} \rightarrow \mathbb{R}^{n_L}\}$, which is basically the collection of all possible functions from $\mathbb{R}^{n_0}$ to $\mathbb{R}^{n_L}$.
Let $p^{in}$ be a fixed probability measure over the input space $\mathbb{R}^{n_0}$.
Then we may consider the input space as a probability space i.e. $X := (\mathbb{R}^{n_0}, \mathcal{L}_{n_0}, p^{in})$ where $\mathcal{B}_k := \mathcal{L}(\mathbb{R}^k)$ is the usual Lebesgue $\sigma$-algebra of $\mathbb{R}^k$.
The output space can be considered as a Banach space with the usual Euclidean norm i.e. $Y = (\mathbb{R}^{n_L}, \lVert \cdot, \rVert)$.

We consider the following subspace of $\mathcal{F}$:
\begin{definition}
	Bochner space $L^p() \subset \mathcal{F}$
\end{definition}
\begin{remark}
	Obviously, $L^p(X; Y)$ is a Banach space...
	
	Really, each element of $L^2(X; Y)$ is an equivalence class...
\end{remark}

Especially when $p = 2$, we have a Hilbert space:
\begin{proposition}
	The function $\langle \cdot, \cdot \rangle_{p^{in}} : L^2(X; Y) \times L^2(X; Y) \rightarrow \mathbb{R}$ defined as below, is an inner product.
	\begin{equation}
		\langle f, g \rangle_{p^{in}} = \mathbb{E}_{x \sim p^{in}} \left[ f(x)^\intercal g(x) \right]
	\end{equation}
	\begin{proof}
		Bilinearity follows directly from the linearity of expectation.
		Symmetry and positive definiteness are trivial.
	\end{proof}
\end{proposition}

Let $\lVert \cdot \rVert_{p^{in}}$ be the induced norm.

Considering the universal approximation properties of ANNs (see Appendix ? for further discussion) and above discussions, we may restrict ourselves to $L^2(X; Y)$, which we shall refer to as $\mathcal{F}$ from heron.
%Also, we assume that $p^{in}$ is the empirical distribution on the given finite training set $\{x_i\}_{i=1}^N \subset \mathbb{R}^{n_0}$ i.e. $p = \frac{1}{N} \sum_{i=1}^N \delta_{x_i}$ where $\delta_x$ is the Dirac measure on $x \in \mathbb{R}^{n_0}$.

%
%\begin{definition}
%	Let $X$ be a real vector space.
%	Then a function $\langle \cdot, \cdot \rangle : X \times X \rightarrow \mathbb{R}$ is a {\bf bilinear form (on $X$)} if it is linear in both arguments i.e. for all $x, y, z \in X$ and $c \in \mathbb{R}$,
%	\begin{equation*}
%		\langle cx + y, z \rangle = c \langle x, z \rangle + \langle y, z \rangle
%	\end{equation*}
%	and
%	\begin{equation*}
%		\langle z, cx + y \rangle = c \langle z, x \rangle + \langle z, y \rangle
%	\end{equation*}
%\end{definition}
%
%\begin{definition}
%	A bilinear form $\langle \cdot, \cdot \rangle : X \times X \rightarrow \mathbb{R}$ is a {\bf semi-inner product (on $X$)} if it is symmetric and positive semi-definite i.e. for all $x, y \in X$, $\langle x, y \rangle = \langle y, x \rangle$ and $\langle x, x \rangle \geq 0$.
%\end{definition}
%
%\begin{theorem}[Cauchy-Schwarz Inequality]
%	Let $\langle \cdot, \cdot \rangle$ be a semi-inner product on $X$.
%	Then for any $x, y \in X$,
%	\begin{equation*}
%		\langle x, y \rangle^2 \leq \langle x, x \rangle \langle y, y \rangle
%	\end{equation*}
%\end{theorem}
%\begin{proof}
%	Exercise to the reader!
%\end{proof}
%
%\begin{definition}
%	A function $p: X \rightarrow \mathbb{R}$ is a {\bf seminorm (on $X$)} if the following properties hold:
%	\begin{enumerate}
%		\item Triangle inequality (Subadditivity):
%		\[ p(x + y) \leq p(x) + p(y) \quad \forall x, y \in X \]
%		
%		\item Absolute homogeneity:
%		\[ p(sx) = |s| p(x) \quad \forall x \in X, s \in \mathbb{F} \]
%	\end{enumerate}
%\end{definition}
%
%\begin{proposition}
%	Given a semi-inner product $\langle \cdot, \cdot \rangle : X \times X \rightarrow \mathbb{R}$, the function $\lVert \cdot \rVert : X \rightarrow \mathbb{R}$ defined as $\lVert x \rVert = \sqrt{\langle x, x \rangle}$ is a seminorm on $X$.
%\end{proposition}
%\begin{proof}
%	Exercise to the reader!
%%	It suffices to check the two properties.
%%	Let $x, y \in X$ and $s \in \mathbb{R}$ be arbitrary.
%%	\begin{enumerate}
%%		\item Triangle inequality
%%		\begin{align*}
%%			\lVert x + y \rVert &= \sqrt{\langle x + y, x + y \rangle} \\
%%			&= \sqrt{\langle x, x \rangle + \langle x, y \rangle + \langle y, x \rangle + \langle y, y \rangle} \\
%%			&= \sqrt{\lVert x \rVert^2 + \lVert y \rVert^2 + 2 \langle x, y \rangle} \\
%%			&= \sqrt{(\lVert x \rVert + \lVert y \rVert)^2 + 2 (\langle x, y \rangle - \lVert x\rVert \lVert y \rVert)}
%%		\end{align*}
%%		
%%		\item Absolute homogeneity:
%%		\begin{equation*}
%%			\lVert sx \rVert = \sqrt{\langle sx, sx \rangle}
%%			= \sqrt{s \langle x, sx \rangle}
%%			= \sqrt{s^2 \langle x, x \rangle}
%%			= |s| \sqrt{\langle x, x \rangle}
%%			= |s| \lVert x \rVert
%%		\end{equation*}
%%	\end{enumerate}
%\end{proof}
%
%\begin{remark}
%	For semi-inner product $\langle \cdot, \cdot \rangle$, $\langle x, x \rangle = 0 \Rightarrow x = 0$ does not hold.
%	For seminorm $p$, $p(x) = 0 \Rightarrow x = 0$ does not hold.
%	With the positive definiteness, we call them inner product and norm, respectively.
%\end{remark}
%
%
%
%
%Observe that this quantifies how {\bf similar} two given functions $f, g \in \mathcal{F}$ are w.r.t. the given input data distribution!


\subsection{Overview}
Before diving into more mathematical details, let us get a grip of the big picture, first.
We usually think of deep learning as optimizing over $\mathbb{R}^P$ and finding some optimal $\theta^*$ with respect to some loss function $C$ i.e.
\begin{equation}
	\theta^* = \argmin_{\theta \in \mathbb{R}^P} C(\theta)
\end{equation}

where $C(\theta)$ is the loss of the neural network constructed using the parameter vector $\theta$.
However, the curse of dimensionality and non-convexity of the loss surface deter us from studying the learning dynamics of deep learning.
Here, we change the viewpoint of optimizing over the parameter space to {\bf optimizing over the function space $\mathcal{F}$}.
In other words, we consider $C$ to be a cost functional\footnote{Functional refers to any mapping that maps a function to a real number.} defined {\it on the function space} i.e. $C : \mathcal{F} \rightarrow \mathbb{R}$ and reformulate the optimization as follows:
\begin{equation}
	\theta^* = \argmin_{\theta \in \mathbb{R}^P} C(f_\theta) = \argmin_{\theta \in \mathbb{R}^P} C \circ F^{(L)}(\theta)
\end{equation}

Here, $F^{(L)} : \mathbb{R}^P \rightarrow \mathcal{F}$ is the ANN realization function that maps the parameter vector $\theta$ to the corresponding neural network $f_\theta$.
In this perspective, we observe that our difficulties mainly arose from the complexity of $F^{(L)}$, and thus, we can {\it exploit the intrinsic structure of $C$ and $\mathcal{F}$ to gain a better understanding of the training dynamics of ANNs}.
%For instance, we will later exploit the convexity of $C$ (convex in $\mathcal{F}$!) directly.

Such exploitation is done in a kernel trick-fashioned way.
Intuitively, we consider {\bf neural network as a feature map}, and we construct the corresponding kernel that can be calculated without knowing what the neural network looks like
With that kernel, we impose a topological structure on $\mathcal{F}$ induced by the kernel (spoiler: semi-inner product space) such that the complex training dynamics of ANN can be described by a simple dynamics on $\mathcal{F}$ w.r.t. that topology.



\section{Kernel Gradient}
We start with a basic definition:
\begin{definition}
	The dual space of $\mathcal{F}$, denoted as $\mathcal{F}^*$, is defined as follows:
	\begin{equation*}
		\mathcal{F}^* = \left\{ \mu = \langle d, \cdot \rangle_{p^{in}} : \mathcal{F} \rightarrow \mathbb{R} \ | \ d \in \mathcal{F} \right\}
	\end{equation*}
\end{definition}

To deal with the cost functional $C : \mathcal{F} \rightarrow \mathbb{R}$, we need to first define the (functional) derivative of $C$.
This is given by the following concept:

\begin{definition}
	Let $V, W$ be Banach spaces, and let $C: V \rightarrow W$ be a function.
	Then $f$ is said to be {\bf Fr\'{e}chet differentiable} at $v \in V$ if there exists a bounded linear operator $A : $
	Fr\'{e}chet derivative
\end{definition}
\begin{remark}
	In our case, $V = \mathcal{F}$
\end{remark}

\begin{theorem}[Riesz representation theorem]
	content...
\end{theorem}

Because $\mathcal{F}$ is a Hilbert space, we have that by Riesz representation theorem, for all $f_0 \in \mathcal{F}$ there exists some corresponding dual element $d \rvert_{f_0} \in \mathcal{F}$ such that $\partial_f^{in} C \rvert_{f_0} = \langle d\rvert_{f_0}, \cdot \rangle_{p^{in}}$ i.e. $\partial_f^{in} C \rvert_{f_0} \in \mathcal{F}^*$.


Now let us introduce kernels:
\begin{definition}
	A function $K : \mathbb{R}^{n_0} \times \mathbb{R}^{n_0} \rightarrow \mathbb{R}^{n_L \times n_L}$ is a {\bf multi-dimensional kernel} if it is a symmetric tensor in $\mathcal{F} \otimes \mathcal{F}$ i.e. $K(x, x') = K(x', x)^\intercal$ for all $x, x' \in \mathbb{R}^{n_0}$.
\end{definition}
\begin{remark}
	The kernel method that we're familiar with (i.e. kernel PCA) deals with the case when $n_L = 1$, which is usually denoted with $k(x, x')$.
	The connection, however, may be deceiving and sometimes lead to serious confusion.
	For instance, since $K(x, x')$ is a matrix, one may try to relate it to the Gram matrix.
	However, Gram matrix is $[k(x_i, x_j)]_{ij}$, while $K(x, x')$ is more like\footnote{Think of it as a cross-similarity matrix in the feature space.} $\phi(x) \phi(x')^\intercal$ where $\phi : \mathbb{R}^{n_0} \rightarrow \mathbb{R}^{n_L}$ is the feature map associated with $K$.
	Gram matrix for $K$, thus, have to be defined in a different way, as we will see later on.
\end{remark}

In this section, let $K$ be a fixed multi-dimensional kernel.


\subsection{Kernel gradient descent}


\begin{theorem}[Chain rule for Fr\'{e}chet derivatives]
\label{thm:chain}
	content...
\end{theorem}

Above equation motivates another Hilbertian structure on $\mathcal{F}$ as follows:
\begin{proposition}
	The function $\langle \cdot, \cdot \rangle_K : \mathcal{F} \times \mathcal{F} \rightarrow \mathbb{R}$, defined as below, is an inner product.
	\begin{equation}
		\langle f, g \rangle_K = \mathbb{E}_{x, x' \sim p^{in}} \left[ f(x)^\intercal K(x, x') g(x') \right]
	\end{equation}
	\begin{proof}
		Bilinearity follows directly from the linearity of expectation.
		Symmetry and positive semi-definiteness are trivial.
	\end{proof}
\end{proposition}

Let $\lVert \cdot \rVert_K$ be the induced norm.
Then Eq. ? can be rewritten as follows:
\begin{equation}
	\partial_t C \rvert_{f(t)} = - \lVert d \rvert_{f(t)} \rVert_K^2.
\end{equation}

Therefore, convergence to a critical point of $C$ is guaranteed if $\lVert d \rvert_{f(t)} \rVert_K > 0$.
But the functional derivative is defined in terms of 


\begin{definition}
	The kernel $K$ is {\bf positive definite} with respect to $\lVert \cdot \rVert_{p^{in}}$ if $\lVert f \rVert_{p^{in}} > 0 \Rightarrow \lVert f \rVert_K > 0$.
\end{definition}



\subsection{Random functions approximation}
\begin{tcolorbox}
	Under linear model assumption, given a multi-dimensional kernel $K$, we shall compute another kernel $\tilde{K}^{(P)}$ such that the ANN realizations follow the kernel gradient descent w.r.t. $\tilde{K}^{(P)}$ under gradient descent dynamics; in the limit $P \rightarrow \infty$, it is shown that $\tilde{K}^{(P)} \rightarrow K$ i.e. the kernel dynamics w.r.t. $\tilde{K}^{(P)}$ is an approximation to the limiting kernel dynamics w.r.t. $K$.
\end{tcolorbox}

Let $\mathcal{D}$ be a fixed distribution on $\mathcal{F}$ whose non-centered covariance is given by $K$ i.e.
\begin{equation}
	\mathbb{E}_{f \sim \mathcal{D}} \left[ f \otimes f \right] 
	= K,
\end{equation}

or equivalently,
\begin{equation}
	\mathbb{E}_{f \sim \mathcal{D}} \left[ f(x) \otimes f(x') \right] 
	= \mathbb{E}_{f \sim \mathcal{D}} \left[ f(x) f(x')^\intercal \right] 
	= K(x, x') \quad x, x' \in X.
\end{equation}

Now sample $\{f^{(p)}\}_{p=1}^P$ from $\mathcal{D}$ uniformly at random.
Then these $P$ functions define a natural random {\it linear} ANN realization function $F^{lin}: \mathbb{R}^P \rightarrow \mathcal{F}$ such that
\begin{equation}
	\theta \mapsto f_\theta^{lin} = \frac{1}{\sqrt{P}} \sum_{p=1}^P \theta_p f^{(p)}
\end{equation}

The $\frac{1}{\sqrt{P}}$ provides the necessary scaling of the final outcome so that we can later apply the law of large number as $P \rightarrow \infty$.
Note that $f_\theta^{lin}$ is almost like a inner product between $\theta = (\theta_p)$ and $f = (f^{(p)})$, with an abuse of notation.

Recall that our goal is to optimize the cost $C \circ F^{lin} : \mathbb{R}^P \rightarrow \mathbb{R}$.
Under gradient descent, each $\theta_p$, which is {\it dependent on $t$}, satisfies $\partial_t \theta_p(t) = -\partial_{\theta_p} (C \circ F^{lin})(\theta(t))$.
Applying \ref{thm:chain} to LHS, we have that
\begin{equation}
	-\partial_{\theta_p} (C \circ F^{lin})(\theta) 
	= - \partial_f^{in} C \rvert_{f_{\theta(t)}^{lin}} \left( \partial_{\theta_p} F^{lin}(\theta) \right)
	= - \left\langle d\rvert_{f_{\theta}^{lin}}, \partial_{\theta_p} F^{lin}(\theta) \right\rangle_{p^{in}}
	= - \frac{1}{\sqrt{P}} \left\langle d\rvert_{f_{\theta}^{lin}}, f^{(p)} \right\rangle_{p^{in}}
\end{equation}
(Dependency on $t$ is temporarily removed; $\theta$ and its components are still functions of $t$)

Now we can finally figure our how $f_{\theta(t)}^{lin}$ evolves w.r.t. $t$:
\begin{equation}
\label{eq:evolve}
	\partial_t f_{\theta(t)}^{lin} = \frac{1}{\sqrt{P}} \sum_{p=1}^P \partial_t \theta_p(t) f^{(p)}
	= - \frac{1}{\sqrt{P}} \sum_{p=1}^P \partial_{\theta_p} (C \circ F^{lin})(\theta(t)) f^{(p)}
	= - \frac{1}{P} \sum_{p=1}^P \left\langle d\rvert_{f_{\theta(t)}^{lin}} , f^{(p)} \right\rangle_{p^{in}} f^{(p)}
\end{equation}

Define the {\bf (linear) tangent kernel at $\theta$, denoted as $\tilde{K}^{(P)}_{\theta(t)}$,} as follows:
\begin{equation}
	\tilde{K}^{(P)}_{\theta} = \sum_{p=1}^P \partial_{\theta_p} F^{lin}(\theta) \otimes \partial_{\theta_p} F^{lin}(\theta)
	= \frac{1}{P} \sum_{p=1}^P f^{(p)} \otimes f^{(p)}
\end{equation}

Because the RHS is independent of $\theta$, we drop the dependency on $\theta$ and write $\tilde{K}^{(P)}$.
Note that by the law of large number, as $P \rightarrow \infty$, we have that $\tilde{K}^{(P)} \rightarrow K$.

Lastly, via a simple computation, we show that the negative of RHS of Eq. \ref{eq:evolve} is exactly the kernel gradient of $C$ w.r.t. $\tilde{K}^{(P)}$ at $f_{\theta(t)}^{lin}$.
Let $x' \in X$ be arbitrary, and let $d = d\rvert_{f_{\theta(t)}^{lin}}$ for simplicity.
Then,
\begin{align*}
	\left( \frac{1}{P} \sum_{p=1}^P \left\langle d, f^{(p)} \right\rangle_{p^{in}} f^{(p)} \right)(x') &= \frac{1}{P} \sum_{p=1}^P \mathbb{E}_{x \sim p^{in}} [d(x)^\intercal f^{(p)}(x)] f^{(p)}(x') \\
	&= \mathbb{E}_{x \sim p^{in}} \left[\frac{1}{P} \sum_{p=1}^P f^{(p)}(x') f^{(p)}(x)^\intercal d(x) \right] \\
	&= \mathbb{E}_{x \sim p^{in}} \left[\tilde{K}^{(P)}(x', x) d(x) \right] \\
	&= \left\langle \tilde{K}^{(P)}(x', \cdot), d \right\rangle_{p^{in}} \\
	&= \left( \Phi_{\tilde{K}^{(P)}} \left( \langle d, \cdot \rangle \right) \right)(x') \\
	&= \left( \Phi_{\tilde{K}^{(P)}} \left( \partial_f^{in} C \rvert_{f_{\theta(t)}^{lin}} \right) \right)(x') \\
	&= \left( \nabla_{\tilde{K}^{(P)}} C \rvert_{f_{\theta(t)}^{lin}} \right)(x')
\end{align*}


\begin{remark}
%	$P \rightarrow \infty$ is basically taking the infinite limit of parameter space.
	This is a reminiscence of \cite{Rahimi-Recht}; there, high-dimensional input data is mapped to a randomized low-dimensional feature space such that the inner product of the transformed data is approximately equal to the given kernel $k$, then fast linear methods were applied for downstream tasks.
	In this case, $P$-dimensional parameter vector is mapped to $\mathcal{F}$ via a random {\it linear} mapping using $P$ random functions, whose tensor(outer) product with itself is approximately the given multi-dimensional kernel $K$.
	To summarize:
	\begin{center}
		\begin{tabular}{ | p{8cm} | p{8cm} |}
			\hline
			\cite{Rahimi-Recht} & \cite{NTK} \\ \hline
			Mapping to low-dimensional feature space $\mathbb{R}^D$ & Mapping to function space $\mathcal{F}$ \\ \hline
			Inner product on $\mathbb{R}^D$ is similar to the given scalar kernel $k$ & Tensor(outer) product on $\mathcal{F}$ is similar to the given multi-dimensional kernel $K$ \\ \hline
			Applying {\it linear} machine after feature mapping & Constructing random {\it linear} ANN realization function \\
			\hline
		\end{tabular}
	\end{center}
	
\end{remark}


\section{Neural tangent kernel}
Using the (almost) exact same reasoning as Section ?.?, we can show that the ANN $f_\theta$ trained using gradient descent on $C \circ F^{(L)}$ evolves along the kernel gradient descent
\begin{equation}
	\partial_t f_{\theta(t)} = -\nabla_{\Theta^{(L)}_{\theta(t)}} C \rvert_{f_{\theta(t)}}
\end{equation}

w.r.t. the {\bf neural tangent kernel (NTK)}, whose definition at $\theta$ is given as following:
\begin{equation}
	\Theta^{(L)}_{\theta} = \sum_{p=1}^P \partial_{\theta_p} F^{(L)}(\theta) \otimes \partial_{\theta_p} F^{(L)}(\theta).
\end{equation}

It should be obvious that $F^{(L)}$ is not linear, and thus, $\partial_{\theta_p} F^{(L)}(\theta)$ is dependent on $\theta$, and so is NTK.

\begin{tcolorbox}
	The NTK is therefore {\bf random} at initialization and {\bf varies} during training.
\end{tcolorbox}

This is not good as such properties deter us from analyzing the training dynamics of ANN further.
However, in the next two subsections, we describe how in the {\bf infinite width limit}, such properties are replaced with much more desirable ones.

\subsection{Initialization}
\begin{tcolorbox}
	In the {\bf (sequential) infinite width limit}, at {\bf random $\mathcal{N}(0, 1)$ initialization}, the {\bf NTK converges to a deterministic limiting kernel}, whose recursive computation involves averaging over some Gaussian processes i.e. random NTK $\rightarrow$ deterministic NTK.
\end{tcolorbox}


\subsection{Training}
\begin{tcolorbox}
	In the {\bf (sequential) infinite width limit}, during {\bf training}, the {\bf NTK stays constant}, and thus gradient descent almost surely converges to some global minimum if the cost is convex (in $\mathcal{F}$) and the NTK is positive definite i.e. varying NTK $\rightarrow$ constant NTK.
\end{tcolorbox}




\section{Case Study: Least-squares regression}



\newpage
\bibliographystyle{alpha}
\bibliography{NTK-bib}

\appendix

\section{Universal Approximation Properties of ANNs}

\end{document}
