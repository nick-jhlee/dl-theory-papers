# Kernel Theory

This folder contains papers that tried to understand deep learning in the lens of kernel theory. This includes Gaussian Process (GP)-based approaches, NTK...etc.



## Random Features for Large-Scale Kernel Machines (Rahimi & Recht, NIPS'07)

- Foundational paper in this topic! (Recipient of NIPS'17 Test-of-Time Award)
- 



## Toward Deeper Understanding of Neural Networks: The Power of Initialization and a Dual View on Expressivity (Daniely et al., NIPS'16)





## To Understand Deep Learning We Need to Understand Kernel Learning (Belkin et al., ICML'18)

- Extended version of the same title by the same authors available on arXiv.



## Deep Neural Networks as Gaussian Processes (Lee & Bahri et al., ICLR'18)



## Neural Tangent Kernel: Convergence and Generalization in Neural Networks (Jacot et al., NIPS'18)



## On Exact Computation with an Infinitely Wide Neural Net (Arora et al., NIPS'19)



## Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent (Lee & Xiao et al., NIPS'19)



## On Lazy Training in Differentiable Programming (Chizat et al., NIPS'19)



## Scaling Limits of Wide Neural Networks with Weight Sharing: Gaussian Process Behavior, Gradient Independence, and Neural Tangent Kernel Derivation (Greg Yang, arXiv'20)



## Deep Equals Shallow for ReLU Networks in Kernel Regimes (Bietti & Bach, ICLR'21)

