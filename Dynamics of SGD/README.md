# Dynamics of SGD

This folder contains papers that tried to understand the dynamics of SGD (and its variants) and its implication on optimization, generalization...etc.



## Directional Analysis of Stochastic Gradient Descent via von Mises-Fisher Distributions in Deep Learning (Lee et al., NIPS'18 Workshop)



## A Tail-Index Analysis of Stochastic Gradient Noise in Deep Neural Networks (Şimşekli et al., ICML'19)



## Hausdorff Dimension, Heavy Tails, and Generalization in Neural Networks (Şimşekli et al., NIPS'20)



## A Diffusion Theory For Deep Learning Dynamics: Stochastic Gradient Descent Exponentially Favors Flat Minima (Xie et al., ICLR'21)



