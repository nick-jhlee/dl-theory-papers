# Dynamics of SGD

This folder contains papers that tried to understand the dynamics of SGD (and its variants) and its implication on optimization, generalization...etc.



## Papers

- Directional Analysis of Stochastic Gradient Descent via von Mises-Fisher Distributions in Deep Learning (Lee et al., *NIPS'18 Workshop*)
  - 
- Stochastic Modified Equations and Dynamics of Stochastic Gradient Algorithms I: Mathematical Foundations (Li et al., *JMLR'19*)
  - 
- A Tail-Index Analysis of Stochastic Gradient Noise in Deep Neural Networks (Şimşekli et al., *ICML'19*)
  - 
- Hausdorff Dimension, Heavy Tails, and Generalization in Neural Networks (Şimşekli et al., *NIPS'20*)
  - 
- The Heavy-Tail Phenomenon in SGD (Gürbüzbalaban et al., arXiv'21)
  - 
- A Diffusion Theory For Deep Learning Dynamics: Stochastic Gradient Descent Exponentially Favors Flat Minima (Xie et al., *ICLR'21*)
  - 
- On the Validity of Modeling SGD with Stochastic Differential Equations (SDEs) (Li et al., *arXiv'21*)
  - 



## Additional Resources

- Stochastic Differential Equations: An Introduction with Applications, 6th Ed. (Bernt Øksendal)
  - Springer Universitext
  - A highly readable (and one of the most widely used) textbook on SDEs, with several applications
- 

